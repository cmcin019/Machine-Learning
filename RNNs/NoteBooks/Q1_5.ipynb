{"cells":[{"cell_type":"markdown","metadata":{"id":"21wIyEdpXb_-"},"source":["#COMP5900 Assignment 2 (Supplementary Materials)\n","Use this code to answer the questions in Assignment 2 Part 1."]},{"cell_type":"markdown","metadata":{"id":"9BoDEkOLJ5Pa"},"source":["# Sentiment Analysis of IMDB Dataset\n","\n","In the following, we'll be building a machine learning model to detect sentiment (i.e. detect if a sentence is positive or negative). This will be done on movie reviews, using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n","\n","\n","We will use LSTM. \n","\n","An easy way of running this Jupiter notebook is to mount your gogle drive so you can use it for loading data and storting the results.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22969,"status":"ok","timestamp":1666458908801,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"2l32uoIr8vaK","outputId":"cb65675e-5164-4d00-a187-76dd189bfe62"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n"]},{"cell_type":"markdown","metadata":{"id":"1etycQz_rknu"},"source":["Change the working directory to where the relevant files are."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":360,"status":"ok","timestamp":1666458909158,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"aJxqgX73v4ct","outputId":"4af631d0-a738-4288-d926-079a3748ff31"},"outputs":[],"source":["# cd 'drive/MyDrive/COMP5900-F22Assignments/Assignment2'"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"CCmNsYBeQ40l"},"outputs":[],"source":["# !ls\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3886,"status":"ok","timestamp":1666458914785,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"icj_Smzh76-y"},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from nltk.corpus import stopwords \n","from collections import Counter\n","import string\n","import re\n","import seaborn as sns\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import torchtext\n","from torchtext.vocab import GloVe"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1666458914787,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"JT8ar-_b79Jj","outputId":"e93bd925-9746-4e11-db2e-9dd39c197aee"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is available\n"]}],"source":["is_cuda = torch.cuda.is_available()\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\")"]},{"cell_type":"markdown","metadata":{"id":"g3XNj_5UsD8Z"},"source":["Load the provided csv file into a panda dataframe. The dataset has a total of 50,000 samples. Often times it helps to work only on a smaller sebset of data and once debuging is done you can run the code on the fukk dataset. For example, df.sample(10000) create a subset of 10,000 samples out of the 50,000 samples."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1708,"status":"ok","timestamp":1666458916477,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"BzRZdZPo79Wp","outputId":"47fa42ee-eb39-41c8-b95d-2eb235c562c1"},"outputs":[{"data":{"text/plain":["(50000, 2)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["base_csv = 'IMDB Dataset.csv'\n","df = pd.read_csv(base_csv)\n","# df=df.sample(10000)\n","df.shape"]},{"cell_type":"markdown","metadata":{"id":"CeMrDCM_s-Mg"},"source":["Here is a few samples of the dataset."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1666458916478,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"xrB29AApwQS0","outputId":"79937eab-15eb-4c37-850f-b592cd7f82f2"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df.head()\n"]},{"cell_type":"markdown","metadata":{"id":"Lt8KlFw-tB2S"},"source":["Target labels are in the form of text (positive or negative. Lets convert that column into a binary variable with values 0 and 1. "]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1666458916479,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"cQGwAbDQwTP6"},"outputs":[],"source":["df['sentiment'].replace({'positive':1, 'negative':0},inplace=True)\n","# df.head()"]},{"cell_type":"markdown","metadata":{"id":"i6SKsWEwrbNB"},"source":["Let's split data into training and validation sets. We first set aside 15% for test. From the remaining we set side 82% for training and the rest for validation. This will roughly split the data 70/15/15."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1666458916479,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"VD0ys1sC8Tcf"},"outputs":[],"source":["X,y = df['review'].values,df['sentiment'].values\n","x_train1, x_test1, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=1)\n","\n","x_train1, x_val1, y_train, y_val = train_test_split(x_train1, y_train, train_size=0.823, random_state=1)\n"]},{"cell_type":"markdown","metadata":{"id":"cfn_OQ3Lt26J"},"source":["We can see how many examples are in each split."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1666458916479,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"yE3_IDAotoZW","outputId":"c57b2cae-c275-4656-983c-1976543e1a1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of training examples: 34977\n","Number of validation examples: 7523\n","Number of test examples: 7500\n"]}],"source":["print(f'Number of training examples: {len(x_train1)}')\n","print(f'Number of validation examples: {len(x_val1)}')\n","print(f'Number of test examples: {len(x_test1)}')"]},{"cell_type":"markdown","metadata":{"id":"ZjcDsrpEulAX"},"source":["Next, we have to build a _vocabulary_. This is a effectively a look up table where every unique word in your data set has a corresponding _index_ (an integer).\n","\n","We do this as our machine learning model cannot operate on strings, only numbers. Each _index_ is used to construct a _one-hot_ vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and dimensionality is the total number of unique words in your vocabulary, commonly denoted by $V$.\n","\n","![](http://people.scs.carleton.ca/~majidkomeili/Teaching/COMP5900-F22/Images/One-hot.png)\n","\n","The number of unique words in our training set is over 100,000, which means that our one-hot vectors will have over 100,000 dimensions! This will make training slow and possibly won't fit onto the GPU. We only keep the top 25,000 most common words. The following builds the vocabulary, only keeping the most common 25,000 tokens.\n","\n","Instead of having our word embeddings initialized randomly, they are initialized with these pre-trained pre-trained word embeddings. We will use [Glove](https://nlp.stanford.edu/projects/glove/).\n","\n","The theory is that these pre-trained vectors already have words with similar semantic meaning close together in vector space, e.g. \"terrible\", \"awful\", \"dreadful\" are nearby. This gives our embedding layer a good initialization as it does not have to learn these relations from scratch."]},{"cell_type":"markdown","metadata":{"id":"0_n2fqlw8j6W"},"source":["Here `6B` indicates these vectors were trained on 6 billion tokens and `100` indicates these vectors are 100-dimensional.\n","\n","**Note**: these vectors are about 862MB, so watch out if you have a limited internet connection. You may use `vectors_cache` to cache it once downloaded. "]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":482,"status":"ok","timestamp":1666458916951,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"sL_MKyK-CRPZ"},"outputs":[],"source":["glove = GloVe(name='6B', dim=100, max_vectors=25000, cache = \"./vectors_cache\") "]},{"cell_type":"markdown","metadata":{"id":"-aggV4jzt0Iz"},"source":[]},{"cell_type":"markdown","metadata":{"id":"vKDKDywR-HMZ"},"source":["We can see the index of a word using `stoi` (**s**tring **to**  **i**nt) method."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1666458916952,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"TfFWgqcxsF5h","outputId":"70319085-b10e-4a36-87d2-70005cce7974"},"outputs":[{"name":"stdout","output_type":"stream","text":["1005\n"]}],"source":["print(glove.stoi['movie'])"]},{"cell_type":"markdown","metadata":{"id":"C5YKDc752rg2"},"source":["and here is the actual embedding of the word"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666458916952,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"INuMXL-stXJe","outputId":"7795966f-b3fe-483c-ce61-801e167e07d9"},"outputs":[{"data":{"text/plain":["tensor([ 0.3825,  0.1482,  0.6060, -0.5153,  0.4399,  0.0611, -0.6272, -0.0254,\n","         0.1643, -0.2210,  0.1442, -0.3721, -0.2168, -0.0890,  0.0979,  0.6561,\n","         0.6446,  0.4770,  0.8385,  1.6486,  0.8892, -0.1181, -0.0125, -0.5208,\n","         0.7785,  0.4872, -0.0150, -0.1413, -0.3475, -0.2959,  0.1028,  0.5719,\n","        -0.0456,  0.0264,  0.5382,  0.3226,  0.4079, -0.0436, -0.1460, -0.4835,\n","         0.3204,  0.5509, -0.7626,  0.4327,  0.6175, -0.3650, -0.6060, -0.7962,\n","         0.3929, -0.2367, -0.3472, -0.6120,  0.5475,  0.9481,  0.2094, -2.7771,\n","        -0.6022,  0.8495,  1.2549,  0.0179, -0.0419,  2.1147, -0.0266, -0.2810,\n","         0.6812, -0.1417,  0.9925,  0.4988, -0.6754,  0.6417,  0.4230, -0.2791,\n","         0.0634,  0.6891, -0.3618,  0.0537, -0.1681,  0.1942, -0.4707, -0.1480,\n","        -0.5899, -0.2797,  0.1679,  0.1057, -1.7601,  0.0088, -0.8333, -0.5836,\n","        -0.3708, -0.5659,  0.2070,  0.0713,  0.0556, -0.2976, -0.0727, -0.2560,\n","         0.4269,  0.0589,  0.0911,  0.4728])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["glove.vectors[1005]"]},{"cell_type":"markdown","metadata":{"id":"jBEYc8RW9_CX"},"source":["We can see the vocabulary directly using `itos` (**i**nt **to**  **s**tring) method."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666458916953,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"I-qxtccRsxBh","outputId":"1bfba939-0eb5-4bde-b39e-c80ac4bcc286"},"outputs":[{"name":"stdout","output_type":"stream","text":["movie\n"]}],"source":["print(glove.itos[1005])"]},{"cell_type":"markdown","metadata":{"id":"bNkFlWhq_OXy"},"source":["## Prepare Data\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1666458916953,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"GPhaR683B-ek"},"outputs":[],"source":["def preprocess_string(s):\n","    # Remove all non-word characters (everything except numbers and letters)\n","    s = re.sub(r\"[^\\w\\s]\", '', s)\n","    # Replace all runs of whitespaces with no space\n","    s = re.sub(r\"\\s+\", '', s)\n","    # replace digits with no space\n","    s = re.sub(r\"\\d\", '', s)\n","\n","    return s"]},{"cell_type":"markdown","metadata":{"id":"Grbtps2X_gJb"},"source":["We keep only the words that are in the vocabulary. "]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":66240,"status":"ok","timestamp":1666458983187,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"DSYXuZ0KvICP"},"outputs":[],"source":["x_train, x_val, x_test = [],[],[]\n","for sent in x_train1:\n","        x_train.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split()\n","                                  if preprocess_string(word) in glove.stoi])\n","\n","for sent in x_val1:        \n","        x_val.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split()\n","                                  if preprocess_string(word) in glove.stoi])  \n","        \n","for sent in x_test1:        \n","        x_test.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split()\n","                                  if preprocess_string(word) in glove.stoi])      "]},{"cell_type":"markdown","metadata":{"id":"3LgAJSov_0r3"},"source":["Now we pad each sample to have a predefined length. For this we clip the long samples and zero-pad shorter onces. Therefore, at the end all samples will have a length of `seq_len`. In our dataset most reviews have less than 500 word. Seo, we set `seq_len` to 500. "]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1666458983189,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"H4ipUZD7Aq4W"},"outputs":[],"source":["def padding_(sentences, seq_len):\n","    features = np.zeros((len(sentences), seq_len),dtype=int)\n","    for ii, review in enumerate(sentences):\n","        if len(review) != 0:\n","            features[ii, -len(review):] = np.array(review)[:seq_len]\n","    return features"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":1011,"status":"ok","timestamp":1666458984173,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"momtcu6aAq9I"},"outputs":[],"source":["x_train_pad = padding_(x_train,500)\n","x_val_pad = padding_(x_val,500)\n","x_test_pad = padding_(x_test,500)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1666458984175,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"B_BmjrXXZ2bs"},"outputs":[],"source":["# create Tensor datasets\n","train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n","valid_data = TensorDataset(torch.from_numpy(x_val_pad), torch.from_numpy(y_val))\n","test_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n","\n","batch_size = 100\n","\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"ihP3iUtW--rv"},"source":["## Build the Model\n","\n","The next stage is building the model that we'll eventually train and evaluate. Our three layers are an _embedding_ layer, our RNN/LSTM, and a _linear_ layer. \n","\n","The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the LSTM, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. We will initialize the embedding layer with GloVe embeddings.\n","\n","![](http://people.scs.carleton.ca/~majidkomeili/Teaching/COMP5900-F22/Images/Embedding.png)\n","\n","\n","The MyRNN layer is our LSTM which takes in our dense vector $x_{t}$ and the previous hidden state $h_{t-1}$, and the previous cell memory $c_{t-1}$ which it uses to calculate the next hidden state, $h_t$.\n","\n","$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n","\n","Thus, the model using an LSTM looks something like (with the embedding layers omitted):\n","\n","![](http://people.scs.carleton.ca/~majidkomeili/Teaching/COMP5900-F22/Images/LSTM.png)\n","\n","The initial cell state, $c_0$, like the initial hidden state is initialized to a tensor of all zeros. \n","\n","Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension.\n","\n","\n","### Bidirectional RNN\n","\n","As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the **last to the first** (a backward RNN). \n","\n","In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor.\n","\n","We make our sentiment prediction using a concatenation of the last hidden state from the forward LSTM (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward LSTM (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=f(h_T^\\rightarrow, h_T^\\leftarrow)$   \n","\n","\n","### Multi-layer RNN\n","\n","In multi-layer LSTM (also called *deep LSTM*) we add additional LSTMs on top of the initial standard LSTM, where each LSTM added is another *layer*. The hidden state output by the first (bottom) LSTM at time-step $t$ will be the input to the LSTM above it at time step $t$. The prediction is then made from the final hidden state of the final (highest) layer.\n","\n","\n","### Implementation Details\n","\n","To use an RNN instead of the LSTM, you can use `nn.RNN` instead of `nn.LSTM`. Also, note that the LSTM returns the `output` and a tuple of the final `hidden` state and the final `cell` state, whereas the standard RNN only returned the `output` and final `hidden` state. \n","\n","As the final hidden state of our LSTM has both a forward and a backward component, which will be concatenated together, the size of the input to the `nn.Linear` layer is twice that of the hidden dimension size.\n","\n","Implementing bidirectionality and adding additional layers are done by passing values for the `num_layers` and `bidirectional` arguments for the RNN/LSTM. \n","\n","The final hidden state, `hidden`, has a shape of _[num layers $\\times$ num directions, batch size, hid dim]_. These are ordered: _[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1]_. As we want the final (top) layer forward and backward hidden states, we get `hidden[2,:,:]` and `hidden[3,:,:]`, and concatenate them together before passing them to the linear layer. \n","\n","The input dimension is the dimension of the one-hot vectors, which is equal to the vocabulary size. \n","\n","The embedding dimension is the size of the dense word vectors i.e. 100.\n","\n","The hidden dimension is the size of the hidden states i.e. 256.\n","\n","The output dimension is usually the number of classes, however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, i.e. a single scalar real number."]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1666458984175,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"7yZ_KM40aBV0"},"outputs":[],"source":["class MyRNN(nn.Module):\n","    def __init__(self):\n","        super(MyRNN,self).__init__()\n"," \n","        self.output_dim = 1        \n","        self.hidden_dim = 200\n","        embedding_dim = 100\n","        bidirectional = True\n","        num_layers = 2\n","      \n","        # We load the word embeddings into the `embedding` layer of our model.\n","        self.embedding = nn.Embedding.from_pretrained(glove.vectors,freeze=True) \n","        \n","        self.lstm = nn.LSTM(input_size=embedding_dim,\n","                            hidden_size=self.hidden_dim,\n","                            num_layers=num_layers,\n","                            bidirectional=bidirectional,\n","                            batch_first=True)\n","        \n","        \n","        self.fc = nn.Linear(self.hidden_dim * 2, self.output_dim)\n","        \n","    def forward(self,x):\n","        embeds = self.embedding(x)  \n","        \n","        output, (hidden, cell) = self.lstm(embeds)\n","        #output = [sent len, batch size, hid dim * 2]        \n","        #hidden = [num layers * 2, batch size, hid dim]\n","        #cell = [num layers * 2, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[2,:,:]) and backward (hidden[3,:,:]) hidden layers\n","        hidden = torch.cat((hidden[2,:,:], hidden[3,:,:]), dim = 1)        \n","        logit = self.fc(hidden)\n","                \n","        return logit"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1666458984175,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"7yZ_KM40aBV0"},"outputs":[],"source":["# Q1.5\n","class MyRNN(nn.Module):\n","    def __init__(self):\n","        super(MyRNN,self).__init__()\n"," \n","        self.output_dim = 1        \n","        self.hidden_dim = 200\n","        embedding_dim = 100\n","        bidirectional = False\n","        num_layers = 1\n","      \n","        # We load the word embeddings into the `embedding` layer of our model.\n","        self.embedding = nn.Embedding.from_pretrained(glove.vectors,freeze=True) \n","        \n","        # self.lstm = nn.LSTM(input_size=embedding_dim,\n","        #                     hidden_size=self.hidden_dim,\n","        #                     num_layers=num_layers,\n","        #                     bidirectional=bidirectional,\n","        #                     batch_first=True)\n","\n","        self.rnn = nn.RNN(input_size=embedding_dim,\n","                            hidden_size=self.hidden_dim,\n","                            num_layers=num_layers,\n","                            bidirectional=bidirectional,\n","                            batch_first=True)\n","        \n","        \n","        self.fc = nn.Linear(self.hidden_dim * 2, self.output_dim)\n","        \n","    def forward(self,x):\n","        embeds = self.embedding(x)  \n","        \n","        output, hidden= self.rnn(embeds)\n","        #output = [sent len, batch size, hid dim * 2]        \n","        #hidden = [num layers * 2, batch size, hid dim]\n","        #cell = [num layers * 2, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[2,:,:]) and backward (hidden[3,:,:]) hidden layers\n","        hidden = torch.cat((hidden[0,:,:], hidden[0,:,:]), dim = 1)   \n","        logit = self.fc(hidden)\n","                \n","        return logit"]},{"cell_type":"markdown","metadata":{"id":"a1Cy8O6pGeRB"},"source":["We now create an instance of our RNN class.\n","We'll print out the total number of parameters in our model. We also print the details of the number of parameters in each layer. For example, ()'lstm.weight_ih_l0', 80000) indicates that there are 100$\\times$200 parameters that connect the embeddings (dim=100) to the hidden layer (dim=200), and there are 3X additional parameters coressponding to the three gates in LSTM. Hence a total of 100$\\times$200$\\times$4=80,000 parameters. Likewise ('lstm.weight_hh_l0', 160000) indicates that there are 200$\\times$200 parameters that connect the previous hidden state (dim=200) to the hidden layer (dim=200), and there are 3X additional parameters coressponding to the three gates in LSTM. Hence a total of 200$\\times$200$\\times$4=160,000 parameters."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1666458984175,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"pmuwfpWdaBYc","outputId":"42970cbe-148f-42d6-b80b-0ef18545f91c"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model has 60,801 trainable parameters\n","MyRNN(\n","  (embedding): Embedding(25000, 100)\n","  (rnn): RNN(100, 200, batch_first=True)\n","  (fc): Linear(in_features=400, out_features=1, bias=True)\n",")\n"]},{"data":{"text/plain":["[('embedding.weight', 2500000),\n"," ('rnn.weight_ih_l0', 20000),\n"," ('rnn.weight_hh_l0', 40000),\n"," ('rnn.bias_ih_l0', 200),\n"," ('rnn.bias_hh_l0', 200),\n"," ('fc.weight', 400),\n"," ('fc.bias', 1)]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["model = MyRNN()\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')\n","print(MyRNN())\n","[(n, p.numel()) for n, p in MyRNN().named_parameters()]"]},{"cell_type":"markdown","metadata":{"id":"gOMTcBkYJLzH"},"source":["## Train the Model"]},{"cell_type":"markdown","metadata":{"id":"3dF7kk4cJNZd"},"source":["Now we'll set up the training and then train the model. First, we'll create an optimizer. This is the algorithm we use to update the parameters of the module. We will use `Adam` instead of `SGD` that we used in Assignment 1. SGD updates all parameters with the same learning rate and choosing this learning rate can be tricky. `Adam` adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates.\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666458984176,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"J2BWXCL9HdvH"},"outputs":[],"source":["import torch.optim as optim\n","optimizer = optim.Adam(model.parameters())"]},{"cell_type":"markdown","metadata":{"id":"6-3D-fDMJw2S"},"source":["Next, we'll define our loss function. In PyTorch this is commonly called a criterion. \n","\n","The loss function here is _binary cross entropy with logits_. Our model currently outputs an unbound real number. As our labels are either 0 or 1, we want to restrict the predictions to a number between 0 and 1. We do this using the _sigmoid_. We then use this bound scalar to calculate the loss using binary cross entropy. \n","\n","The `BCEWithLogitsLoss` criterion carries out both the sigmoid and the binary cross entropy steps.\n","\n","Using `.to`, we can place the model and the criterion on the GPU (if we have one). "]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7199,"status":"ok","timestamp":1666458991365,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"5cPsvn6DJytx","outputId":"20bf9b02-530b-4474-e0e7-282365dab433"},"outputs":[{"name":"stdout","output_type":"stream","text":["MyRNN(\n","  (embedding): Embedding(25000, 100)\n","  (rnn): RNN(100, 200, batch_first=True)\n","  (fc): Linear(in_features=400, out_features=1, bias=True)\n",")\n"]}],"source":["criterion = nn.BCEWithLogitsLoss()\n","\n","model.to(device)\n","criterion = criterion.to(device)\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"iTGRZrJ7J-Yu"},"source":["Our criterion function calculates the loss, however we have to write our function to calculate the accuracy. \n","\n","This function first feeds the predictions through a sigmoid layer, squashing the values between 0 and 1, we then round them to the nearest integer. This rounds any value greater than 0.5 to 1 (a positive sentiment) and the rest to 0 (a negative sentiment).\n","\n","We then calculate how many rounded predictions equal the actual labels and average it across the batch."]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1666458991366,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"MmaEBXFTw0Zg"},"outputs":[],"source":["def binary_accuracy(preds, y):\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float()  \n","    acc = correct.sum() / len(correct)\n","    return acc"]},{"cell_type":"markdown","metadata":{"id":"BZhT9loCKW1z"},"source":["The `train` function iterates over all examples, one batch at a time. \n","\n","`model.train()` is used to put the model in \"training mode\", which turns on _dropout_ and _batch normalization_. Though we aren't using them in this model.\n","\n","For each batch, we first zero the gradients. Each parameter in a model has a `grad` attribute which stores the gradient calculated by the `criterion`. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n","\n","We then feed the batch of sentences, `inputs`, into the model. Note, you do not need to do `model.forward(inputs)`, simply calling the model works. The `squeeze` is needed as the predictions are initially size _[batch size, 1]_, and we need to remove the dimension of size 1 as PyTorch expects the predictions input to our criterion function to be of size _[batch size]_.\n","\n","The loss and accuracy are then calculated using our predictions and the labels, `labels`, with the loss being averaged over all examples in the batch."]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1666458991366,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"hV3MX6Zgxoyp"},"outputs":[],"source":["def train(model, data_loader, optimizer, criterion):\n","  epoch_loss = 0\n","  epoch_acc = 0\n","  model.train()\n","\n","  for inputs, labels in train_loader:\n","      optimizer.zero_grad()\n","      inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float)   \n","      predictions = model(inputs).squeeze(1)      \n","      loss = criterion(predictions, labels)      \n","      loss.backward()\n","      optimizer.step()\n","      acc = binary_accuracy(predictions, labels)\n","      epoch_loss += loss.item()\n","      epoch_acc += acc.item()\n","      \n","  return epoch_loss / len(data_loader), epoch_acc / len(data_loader)"]},{"cell_type":"markdown","metadata":{"id":"G6IjeopkLX-Q"},"source":["`evaluate` is similar to `train`, with a few modifications as you don't want to update the parameters when evaluating.\n","\n","`model.eval()` puts the model in \"evaluation mode\", this turns off _dropout_ and _batch normalization_. Though, we are not using them in this model.\n","\n","No gradients are calculated on PyTorch operations inside the `with no_grad()` block. This causes less memory to be used and speeds up computation.\n","\n","The rest of the function is the same as `train`, with the removal of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as we do not update the model's parameters when evaluating.\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1666458991367,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"BvsNRB2a5BEe"},"outputs":[],"source":["def evaluate(model, data_loader, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float)\n","            predictions = model(inputs).squeeze(1)\n","            loss = criterion(predictions, labels)\n","            acc = binary_accuracy(predictions, labels)\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","    return epoch_loss / len(data_loader), epoch_acc / len(data_loader)"]},{"cell_type":"markdown","metadata":{"id":"d-j4S7AnLxn_"},"source":["And also create a nice function to tell us how long our epochs are taking."]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1666458991367,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"MN7jahSD4gXb"},"outputs":[],"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"markdown","metadata":{"id":"ep8uPqbZL1j8"},"source":["Finally, we train our model...\n","At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":419775,"status":"ok","timestamp":1666459411113,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"j-22tZ4XBHsY","outputId":"f389783d-c270-4072-a97f-1e55ceabdd9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.67174 | Train Acc: 58.71%\n","\t Val. Loss: 0.66053 |  Val. Acc: 60.07%\n","Epoch: 02 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.65992 | Train Acc: 60.46%\n","\t Val. Loss: 0.63469 |  Val. Acc: 64.51%\n","Epoch: 03 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.65782 | Train Acc: 59.77%\n","\t Val. Loss: 0.67443 |  Val. Acc: 58.03%\n","Epoch: 04 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.64967 | Train Acc: 61.98%\n","\t Val. Loss: 0.65467 |  Val. Acc: 60.90%\n","Epoch: 05 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.63994 | Train Acc: 63.59%\n","\t Val. Loss: 0.64763 |  Val. Acc: 62.68%\n"]}],"source":["N_EPOCHS = 5\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'best-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')"]},{"cell_type":"markdown","metadata":{"id":"65cNMkN-R6sz"},"source":["Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss."]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5601,"status":"ok","timestamp":1666459416685,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"d1KK_FHVxo1k","outputId":"00612d34-bdd9-4885-b44f-ae53ee7515d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 0.634 | Test Acc: 64.55%\n"]}],"source":["model.load_state_dict(torch.load('best-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_loader, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"]},{"cell_type":"markdown","metadata":{"id":"dYTqXJ21THYa"},"source":["## User Input\n","\n","We can now use our model to predict the sentiment of any sentence we give it.\n","\n","Our `predict_sentiment` function does a few things:\n","- sets the model to evaluation mode\n","- tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n","- indexes the tokens by converting them into their integer representation from our vocabulary\n","- converts the indexes, which are a Python list into a PyTorch tensor\n","- add a batch dimension by `unsqueeze`ing \n","- squashes the output prediction to a real number between 0 and 1 with the `sigmoid` function\n","- converts the tensor holding a single value into an Python number with the `item()` method\n","\n","We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1666459416686,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"9-fXls1bK2lG"},"outputs":[],"source":["def predict_sentiment(model, sentence):\n","    model.eval()\n","    #x_train.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split()\n","                                  #if preprocess_string(word) in glove.stoi])\n","    input = [glove.stoi[preprocess_string(word)] for word in sentence.lower().split()\n","                                  if preprocess_string(word) in glove.stoi]\n","    tensor = torch.LongTensor(input).to(device)\n","    tensor = tensor.unsqueeze(0)\n","    logit = model(tensor)\n","    prediction = torch.sigmoid(logit)\n","    return prediction.item()"]},{"cell_type":"markdown","metadata":{"id":"KG6uqJ5VYC18"},"source":["An example negative review..."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1666459416687,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"bC2bk_ViK2n9","outputId":"b533b273-1723-45af-e67a-39525edc0105"},"outputs":[{"data":{"text/plain":["0.25653448700904846"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["predict_sentiment(model, \"This film is terrible\")"]},{"cell_type":"markdown","metadata":{"id":"rxmDk-g7J5RX"},"source":["An example positive review..."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1666459416687,"user":{"displayName":"Majid Komeili","userId":"09257784370227023951"},"user_tz":240},"id":"XqU0LCgMJ5RY","outputId":"d08dee8f-2959-4799-80d6-58e75c9fb54b"},"outputs":[{"data":{"text/plain":["0.6131161451339722"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["predict_sentiment(model, \"This film is great\")"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["0.5333080887794495"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["predict_sentiment(model, \"The story was not amazing, and the visuals were not the best\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.12 ('torch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"vscode":{"interpreter":{"hash":"307be5a38ff2ed762a41e330339f491ffb10c94b9424293f4d0878b2873917ca"}}},"nbformat":4,"nbformat_minor":0}
